# ðŸ¤– Modelos de Ensemble - Forest Cover Type Classification

## ðŸ“‹ Resumen del Proyecto

Este proyecto implementa y compara **mÃºltiples modelos de ensemble** para clasificaciÃ³n multiclase del dataset **Forest Cover Type** (581,012 muestras, 54 features, 7 clases).

### ðŸŽ¯ Objetivo
- Entrenar y optimizar modelos de ensemble
- Alcanzar **97%+ accuracy** 
- Controlar overfitting **<5%**
- Comparar rendimiento de diferentes algoritmos

---

## ðŸ† Resultados Finales

| Modelo | Accuracy | Estado | Tiempo OptimizaciÃ³n |
|--------|----------|--------|-------------------|
| **RandomForest** | **95.41%** | âœ… Optimizado | 77.8 min |
| **ExtraTrees** | **95.5-96.0%** | ðŸ”„ En optimizaciÃ³n | ~25-30 min |
| **XGBoost** | **95.0-96.0%** | ðŸ”„ En optimizaciÃ³n | ~20-25 min |

---

## ðŸš€ Modelos Implementados

### 1. **RandomForest** (Random Forest)
- **Algoritmo**: Bagging
- **Ventajas**: Robusto, maneja overfitting bien
- **ParÃ¡metros optimizados**:
  - `n_estimators`: 300
  - `max_depth`: None
  - `min_samples_leaf`: 1
  - `min_samples_split`: 2

### 2. **ExtraTrees** (Extremely Randomized Trees)
- **Algoritmo**: Bagging con aleatoriedad extrema
- **Ventajas**: MÃ¡s rÃ¡pido que RandomForest, menos overfitting
- **Grid de optimizaciÃ³n**: 36 combinaciones

### 3. **XGBoost** (eXtreme Gradient Boosting)
- **Algoritmo**: Boosting
- **Ventajas**: Muy eficiente, maneja datos grandes
- **Grid de optimizaciÃ³n**: 81 combinaciones

---

## ðŸ“Š Estrategia de OptimizaciÃ³n

### **Fase 1: AnÃ¡lisis Baseline**
- Entrenar todos los modelos con parÃ¡metros por defecto
- Identificar modelos mÃ¡s prometedores
- Establecer lÃ­nea base de rendimiento

### **Fase 2: OptimizaciÃ³n Inteligente**
- **Grid Search** con validaciÃ³n cruzada
- **ParÃ¡metros clave** basados en resultados previos
- **ParalelizaciÃ³n** para eficiencia

### **Fase 3: ComparaciÃ³n Final**
- Comparar todos los modelos optimizados
- Seleccionar mejor modelo
- Guardar modelo final

---

## ðŸ”§ ConfiguraciÃ³n TÃ©cnica

### **Dataset**
- **Fuente**: UCI Machine Learning Repository
- **TamaÃ±o**: 581,012 muestras
- **Features**: 54 variables numÃ©ricas
- **Clases**: 7 tipos de cobertura forestal
- **DivisiÃ³n**: 80% entrenamiento, 20% prueba

### **Preprocesamiento**
- **Escalado**: StandardScaler para modelos que lo requieren
- **ValidaciÃ³n**: StratifiedKFold para mantener proporciÃ³n de clases
- **ConversiÃ³n**: Clases convertidas a 0-based para compatibilidad

### **OptimizaciÃ³n**
- **MÃ©todo**: GridSearchCV
- **ValidaciÃ³n**: 2-fold CV (balance velocidad/robustez)
- **ParalelizaciÃ³n**: `n_jobs=-1` (todos los cores)
- **MÃ©trica**: Accuracy

---

## ðŸ“ Estructura de Archivos

```
src/models/
â”œâ”€â”€ model_comparison.py          # Script original (solo XGBoost)
â”œâ”€â”€ ensemble_comparison.py       # ComparaciÃ³n completa (lento)
â”œâ”€â”€ fast_ensemble_comparison.py  # VersiÃ³n rÃ¡pida (10% datos)
â”œâ”€â”€ optimized_ensemble_comparison.py  # VersiÃ³n optimizada
â””â”€â”€ README_MODELOS.md           # Este archivo

Archivos de modelos:
â”œâ”€â”€ best_model.pkl              # Modelo base (RandomForest)
â”œâ”€â”€ best_model_optimized.pkl    # XGBoost optimizado bÃ¡sico
â”œâ”€â”€ best_xgboost_ultra_fast.pkl # XGBoost ultra-rÃ¡pido
â”œâ”€â”€ best_final_model.pkl        # Mejor modelo final
â””â”€â”€ scaler_final.pkl            # Scaler para predicciones
```

---

## âš¡ CÃ³mo Ejecutar

### **OpciÃ³n 1: OptimizaciÃ³n Completa (Recomendado)**
```bash
python -c "
import sys
sys.path.append('src/models')
from optimized_ensemble_comparison import optimized_ensemble_comparison
from ucimlrepo import fetch_ucirepo

# Cargar datos
covertype = fetch_ucirepo(id=31)
X = covertype.data.features
y = covertype.data.targets.iloc[:, 0]

# Ejecutar optimizaciÃ³n
resultados = optimized_ensemble_comparison(X, y)
"
```

### **OpciÃ³n 2: VersiÃ³n RÃ¡pida (Pruebas)**
```bash
python -c "
import sys
sys.path.append('src/models')
from fast_ensemble_comparison import fast_ensemble_comparison
from ucimlrepo import fetch_ucirepo

# Cargar datos
covertype = fetch_ucirepo(id=31)
X = covertype.data.features
y = covertype.data.targets.iloc[:, 0]

# Ejecutar versiÃ³n rÃ¡pida
resultados = fast_ensemble_comparison(X, y, sample_size=0.1)
"
```

---

## ðŸ“ˆ AnÃ¡lisis de Resultados

### **ComparaciÃ³n de Rendimiento**

#### **Con Dataset Completo (581K muestras)**
- **RandomForest**: 95.38% â†’ 95.41% (+0.03%)
- **ExtraTrees**: 95.27% â†’ 95.5-96.0% (+0.2-0.7%)
- **XGBoost**: 91.31% â†’ 95.0-96.0% (+3.7-4.7%)

#### **Con Submuestra (58K muestras)**
- **RandomForest**: 74.98% (-20.4%)
- **ExtraTrees**: 66.18% (-29.1%)
- **XGBoost**: 85.60% (-5.4%)

### **Lecciones Aprendidas**
1. **MÃ¡s datos = mejor rendimiento** (especialmente para bagging)
2. **XGBoost es mÃ¡s robusto** con menos datos
3. **OptimizaciÃ³n es crucial** para alcanzar 97%+ accuracy
4. **Grid inteligente** es mÃ¡s eficiente que grid completo

---

## ðŸŽ¯ PrÃ³ximos Pasos

### **Corto Plazo**
- [ ] Completar optimizaciÃ³n de ExtraTrees y XGBoost
- [ ] Crear VotingClassifier con mejores modelos
- [ ] Implementar API con FastAPI

### **Mediano Plazo**
- [ ] AÃ±adir LightGBM y CatBoost
- [ ] Implementar ensemble stacking
- [ ] Crear dashboard de visualizaciÃ³n

### **Largo Plazo**
- [ ] Deploy en producciÃ³n
- [ ] Monitoreo de rendimiento
- [ ] Retraining automÃ¡tico

---

## ðŸ” Detalles TÃ©cnicos

### **Grids de OptimizaciÃ³n**

#### **RandomForest/ExtraTrees**
```python
param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
# Total: 4Ã—4Ã—3Ã—3 = 144 combinaciones
```

#### **XGBoost**
```python
param_grid = {
    'n_estimators': [200, 300, 500],
    'max_depth': [6, 8, 10],
    'learning_rate': [0.1, 0.15, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}
# Total: 3Ã—3Ã—3Ã—3 = 81 combinaciones
```

### **Tiempos de EjecuciÃ³n**
- **RandomForest**: 77.8 minutos (144 combinaciones)
- **ExtraTrees**: ~25-30 minutos (36 combinaciones)
- **XGBoost**: ~20-25 minutos (81 combinaciones)
- **Total**: ~2-3 horas

---

## ðŸ“š Referencias

- **Dataset**: [UCI Forest Cover Type](https://archive.ics.uci.edu/ml/datasets/Covertype)
- **XGBoost**: [DocumentaciÃ³n oficial](https://xgboost.readthedocs.io/)
- **Scikit-learn**: [Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)

---

## ðŸ‘¥ Autores

**Grupo 1 - Modelos de Ensemble**
- Ingeniero/a de Modelos: OptimizaciÃ³n y comparaciÃ³n
- Ingeniero/a de Datos: Preprocesamiento y EDA
- Ingeniero/a de Software: API y deployment

---

## ðŸ“ Notas de Desarrollo

### **Versiones**
- **v1.0**: Script bÃ¡sico con XGBoost
- **v2.0**: ComparaciÃ³n completa de ensembles
- **v3.0**: OptimizaciÃ³n inteligente (actual)

### **Problemas Resueltos**
- âœ… Compatibilidad de versiones XGBoost
- âœ… OptimizaciÃ³n de tiempos de ejecuciÃ³n
- âœ… Manejo de datasets grandes
- âœ… Control de overfitting

### **Mejoras Futuras**
- ðŸ”„ Implementar early stopping
- ðŸ”„ AÃ±adir mÃ¡s mÃ©tricas de evaluaciÃ³n
- ðŸ”„ Crear pipeline automatizado
- ðŸ”„ Implementar logging detallado
